# ğŸ”Œ Offline Setup Guide

This project can run **100% OFFLINE** after initial model download.

## ğŸ“‹ Prerequisites

- Python 3.8+
- pip
- **Internet connection ONLY for first-time setup**

---

## ğŸš€ First-Time Setup (Requires Internet)

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Download All Models (ONE TIME ONLY)

Run this script **once** with internet to download all models:

```bash
python download_models.py
```

This will download (~500MB):
- âœ… Sentence Transformer (all-MiniLM-L6-v2) â†’ embedding model
- âœ… BLIP Image Captioning â†’ image understanding

Models are cached in:
- `./model_cache/` (local project cache)
- `~/.cache/huggingface/` (system cache)

### Step 3: Upload Sample Documents (Optional)

Use the web UI to upload PDF/DOCX/MD/TXT documents to populate your knowledge base.

---

## ğŸŒ Running Offline

After initial setup, **disconnect from internet** and run:

```bash
python api_server.py
```

Then open: **http://127.0.0.1:8000**

### âœ… What Works Offline:
- âœ… FastAPI server startup
- âœ… All UI assets (local CSS/JS/fonts)
- âœ… Document ingestion (PDF, Word, Markdown, Text, RTF)
- âœ… Vector search with FAISS
- âœ… LLM inference (if using local llama.cpp)
- âœ… Image captioning with BLIP
- âœ… Full chat and reasoning pipeline

### âŒ What Requires Internet (if not pre-downloaded):
- âŒ First-time model downloads
- âŒ CDN resources (now replaced with local files)

---

## ğŸ“¦ Sharing the Project

To share this project with someone else:

1. **Zip the entire folder** including:
   - All Python files
   - `./model_cache/` folder (if you want to include models)
   - `./static/` folder
   - `requirements.txt`

2. **On the new machine**, recipient should:
   - Extract the ZIP
   - Install dependencies: `pip install -r requirements.txt`
   - If models NOT included in ZIP, run: `python download_models.py` (requires internet)
   - Start server: `python api_server.py`

---

## ğŸ”§ Troubleshooting

### Issue: "Model not found" error on startup
**Solution:** Run `python download_models.py` with internet

### Issue: UI assets not loading
**Solution:** Check that `./static/tailwind.min.css`, `./static/fontawesome.min.css`, and `./static/d3.min.js` exist

### Issue: LLM server offline
**Solution:** Make sure llama.cpp server is running on `http://127.0.0.1:8080` or configure fallback

---

## ğŸ“ Project Structure (Offline-Ready)

```
Multi_agent/
â”œâ”€â”€ api_server.py          # FastAPI backend
â”œâ”€â”€ main_engine.py         # Orchestrator
â”œâ”€â”€ vector_store.py        # FAISS + embeddings
â”œâ”€â”€ image_captioner.py     # BLIP captioning
â”œâ”€â”€ ingestion.py           # Document processing
â”œâ”€â”€ download_models.py     # One-time model downloader
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ model_cache/           # Local model cache (created on first run)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html         # Main UI
â”‚   â”œâ”€â”€ tailwind.min.css   # Offline Tailwind
â”‚   â”œâ”€â”€ fontawesome.min.css # Offline Font Awesome
â”‚   â””â”€â”€ d3.min.js          # Offline D3 (minimal)
â”œâ”€â”€ faiss_db/              # Vector store persistence
â”œâ”€â”€ data/                  # Uploaded documents
â””â”€â”€ cache.db               # Query cache
```

---

## âœ¨ Fully Offline Operation Confirmed

- ğŸŒ **No CDN dependencies** (Tailwind, Font Awesome, D3 are local)
- ğŸ¤– **No model downloads** (after first run)
- ğŸ”’ **No internet required** for normal operation
- ğŸ“¦ **Portable** - zip and share with others

---

**You can now run this project on an air-gapped machine!** ğŸ‰
