ğŸ” Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ Multi (Agentic Research Assistant)
ğŸ“Š Ø³Ø§Ø®ØªØ§Ø± Ú©Ù„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
Ù¾Ø±ÙˆÚ˜Ù‡ Ø´Ù…Ø§ ÛŒÚ© RAG-based Multi-Agent Research Assistant Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§ÛŒÙ†â€ŒØ·ÙˆØ±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

LLM: Llama-3-8B-Instruct (via llama.cpp server, port 8080)

Vector DB: FAISS + Sentence Transformers (all-MiniLM-L6-v2)

Backend: FastAPI (port 8000)

Agents: Query Understanding â†’ Hybrid Retrieval â†’ Reasoning â†’ Verification

Features: PDF ingestionØŒ image extractionØŒ multilingual (FAâ†’EN)ØŒ cachingØŒ export

ğŸ› Ø¨Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ (Critical)
1. Cache Layer ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª âš ï¸
Ù…Ú©Ø§Ù†: main_engine.py Ø®Ø·ÙˆØ· 178-187ØŒ 376-379

Ù…Ø´Ú©Ù„: Ú©Ø¯ caching Ú©Ø§Ù…Ù„ Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡:

python
# cached_response = self.cache.get(user_query)
# if cached_response:
#     ...
# self.cache.set(user_query, response)
ØªØ£Ø«ÛŒØ±: Ù‡Ø± query Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø² ØµÙØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯. response time 30-40s Ø¨Ø±Ø§ÛŒ queries ØªÚ©Ø±Ø§Ø±ÛŒ.

Ø±Ø§Ù‡Ú©Ø§Ø±:

python
# Uncomment cache logic Ø¯Ø± Ø®Ø· 178 Ùˆ 376
# Ø§Ù…Ø§ Ø§ÙˆÙ„ Ø¨Ø§ÛŒØ¯ bug Ù‡Ø§ÛŒ cache.py Ø±Ùˆ fix Ú©Ù†ÛŒ
2. Conflict Ø¯Ø± Vector Store âš ï¸
Ù…Ú©Ø§Ù†: vector_store.py vs Ù…Ø³ØªÙ†Ø¯Ø§Øª

Ù…Ø´Ú©Ù„:

Ú©Ø¯ Ø§Ø² FAISS Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡

Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ÛŒâ€ŒÚ¯Ù‡ ChromaDB

ØªØ£Ø«ÛŒØ±: FAISS Ø³Ø§Ø¯Ù‡â€ŒØªØ±Ù‡ Ø§Ù…Ø§ ChromaDB features Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ù‡ (metadata filteringØŒ updatesØŒ etc.)

Ø±Ø§Ù‡Ú©Ø§Ø±: Ø§Ú¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ stay Ø¨Ø§ FAISSØŒ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ùˆ update Ú©Ù†. Ø§Ú¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ ChromaDBØŒ Ø¨Ø§ÛŒØ¯ migration Ú©Ù†ÛŒ.

3. requirements.txt Ù†Ø§Ù‚Øµ Ø§Ø³Øª ğŸš¨
Ù…Ú©Ø§Ù†: requirements.txt

Ù…Ø´Ú©Ù„: dependencies Ø­ÛŒØ§ØªÛŒ missing Ù‡Ø³ØªÙ†Ø¯:

text
âŒ fastapi
âŒ uvicorn
âŒ pymupdf (fitz import Ø¯Ø± ingestion.py)
âŒ googletrans ÛŒØ§ deep-translator
âŒ httpx (Ø¨Ø±Ø§ÛŒ async health check)
âŒ pdfplumber (Ø¨Ø±Ø§ÛŒ table extraction - Ø¯Ø± FIX.md Ø°Ú©Ø± Ø´Ø¯Ù‡)
âŒ python-multipart (Ø¨Ø±Ø§ÛŒ file upload)
Ø±Ø§Ù‡Ú©Ø§Ø±:

bash
pip install fastapi uvicorn pymupdf googletrans==4.0.0rc1 httpx pdfplumber python-multipart
Ø¨Ø¹Ø¯Ø´ update Ú©Ù†:

text
echo "fastapi==0.109.0" >> requirements.txt
echo "uvicorn==0.27.0" >> requirements.txt
echo "pymupdf==1.23.8" >> requirements.txt
echo "googletrans==4.0.0rc1" >> requirements.txt
echo "httpx==0.26.0" >> requirements.txt
echo "pdfplumber==0.10.3" >> requirements.txt
echo "python-multipart==0.0.6" >> requirements.txt
4. Translation Fallback Ø¶Ø¹ÛŒÙ âš ï¸
Ù…Ú©Ø§Ù†: main_engine.py Ø®Ø·ÙˆØ· 102-132

Ù…Ø´Ú©Ù„: Ø§Ú¯Ù‡ googletrans fail Ø¨Ø´Ù‡ØŒ query Ø§ØµÙ„ÛŒ return Ù…ÛŒâ€ŒØ´Ù‡ Ø¨Ø¯ÙˆÙ† translation:

python
except Exception as e:
    print(f"Translation API error: {e}")
return query, original_lang  # â† Ø§Ú¯Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨ÙˆØ¯ØŒ untranslated Ù…ÛŒâ€ŒÙ…ÙˆÙ†Ù‡
ØªØ£Ø«ÛŒØ±: queries ÙØ§Ø±Ø³ÛŒ fail Ù…ÛŒâ€ŒØ´Ù† Ú†ÙˆÙ† LLM Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒâ€ŒØ³Øª.

Ø±Ø§Ù‡Ú©Ø§Ø±:

python
# Add fallback Ø¨Ù‡ deep-translator ÛŒØ§ hardcoded dictionary Ø¨Ø±Ø§ÛŒ technical terms
from deep_translator import GoogleTranslator

if TRANSLATION_AVAILABLE and TRANSLATOR:
    try:
        result = TRANSLATOR.translate(query, dest='en')
        ...
    except:
        # Fallback 2: deep-translator
        try:
            result = GoogleTranslator(source='fa', target='en').translate(query)
            return result, original_lang
        except:
            # Fallback 3: Manual mapping of common terms
            return self._manual_translate(query), original_lang
5. LLM Client Ø¨Ø¯ÙˆÙ† Timeout/Retry ğŸš¨
Ù…Ú©Ø§Ù†: llm_client.py Ø®Ø·ÙˆØ· 90-110

Ù…Ø´Ú©Ù„: requests.post timeout=60 Ø¯Ø§Ø±Ù‡ Ø§Ù…Ø§:

Ø§Ú¯Ù‡ LLM hang Ø¨Ø´Ù‡ØŒ 60s wait Ù…ÛŒâ€ŒÚ©Ù†Ù‡

Ø§Ú¯Ù‡ connection error Ø¨Ø´Ù‡ØŒ retry Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù‡

Ø§Ú¯Ù‡ response Ø®Ø§Ù„ÛŒ Ø¨ÛŒØ§Ø¯ØŒ validate Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù‡

ØªØ£Ø«ÛŒØ±: 25% reasoning failures (Ø·Ø¨Ù‚ FIX.md)

Ø±Ø§Ù‡Ú©Ø§Ø± (Ø·Ø¨Ù‚ FIX.md):

python
def generate(self, prompt: str, max_tokens=400, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(
                f"{self.base_url}/completion",
                json={...},
                timeout=30  # Ú©Ø§Ù‡Ø´ Ø§Ø² 60 Ø¨Ù‡ 30
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result.get("content", "").strip()
                
                # Validate response
                if not text or len(text) < 20:
                    raise ValueError("Empty or too short response")
                
                return {"success": True, "text": text}
            
        except (requests.Timeout, requests.ConnectionError) as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            return {"success": False, "error": f"Max retries exceeded: {e}"}
        
        except Exception as e:
            return {"success": False, "error": str(e)}
6. ReasoningAgent Fallback Ù†Ø§Ù‚Øµ âš ï¸
Ù…Ú©Ø§Ù†: agents/specific_agents.py Ø®Ø·ÙˆØ· 174-280

Ù…Ø´Ú©Ù„: fallback mechanism Ù‡Ø³Øª Ø§Ù…Ø§:

_simplified_reasoning context Ø±Ùˆ truncate Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ø¨Ù‡ 2000 chars Ø§Ù…Ø§ token count check Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù‡

_direct_extraction sentence matching Ø³Ø§Ø¯Ù‡â€ŒØ³ØŒ technical queries Ø±Ùˆ handle Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù‡

Log file Ø¯Ø± logs/reasoning_failures.log save Ù…ÛŒâ€ŒØ´Ù‡ Ø§Ù…Ø§ Ø§ÛŒÙ† directory Ù…Ù…Ú©Ù† Ø§Ø³Øª exist Ù†Ú©Ù†Ù‡

Ø±Ø§Ù‡Ú©Ø§Ø±:

python
# Add Ø¯Ø± __init__:
os.makedirs('logs', exist_ok=True)

# Add token estimation:
def _estimate_tokens(self, text: str) -> int:
    return int(len(text.split()) * 1.3)  # Rough estimation

def _simplified_reasoning(self, query: str, context_text: str):
    max_tokens = 2048 - 200 - 100 - 500  # system + query + response
    while self._estimate_tokens(context_text) > max_tokens:
        context_text = context_text[:int(len(context_text) * 0.8)]
    
    # Rest of code...
7. Image Captioner Import Error Silent Fail âš ï¸
Ù…Ú©Ø§Ù†: api_server.py Ø®Ø·ÙˆØ· 82-87

Ù…Ø´Ú©Ù„: Ø§Ú¯Ù‡ BLIP model load Ù†Ø´Ù‡ØŒ image captioning disable Ù…ÛŒâ€ŒØ´Ù‡ Ø§Ù…Ø§:

User Ù…ØªÙˆØ¬Ù‡ Ù†Ù…ÛŒâ€ŒØ´Ù‡

Images Ø¨Ø¯ÙˆÙ† caption index Ù…ÛŒâ€ŒØ´Ù† â†’ retrieval Ø¶Ø¹ÛŒÙ

Ø±Ø§Ù‡Ú©Ø§Ø±:

python
# Add health status endpoint
@app.get("/api/health/detailed")
async def detailed_health():
    return {
        "llm_status": llm_client.health_check(),
        "multimodal_status": llm_client.multimodal_health_check(),
        "image_captioner": image_captioner is not None,  # â† Add this
        "vector_db_count": vector_store.get_collection_count()
    }
Ùˆ Ø¯Ø± frontend Ù†Ø´ÙˆÙ† Ø¨Ø¯Ù‡ Ú©Ù‡ image captioning off Ø§Ø³Øª.

8. Session Endpoints Without SESSION_SUPPORT âš ï¸
Ù…Ú©Ø§Ù†: api_server.py Ø®Ø·ÙˆØ· 399-446

Ù…Ø´Ú©Ù„: session endpoints Ù‡Ù…ÛŒØ´Ù‡ exist Ù‡Ø³ØªÙ†Ø¯ Ø§Ù…Ø§:

python
if not SESSION_SUPPORT:
    return {"id": f"temp_{id(title)}", ...}  # Temp fallback
ØªØ£Ø«ÛŒØ±: Frontend ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡ session Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ø§Ù…Ø§ data persist Ù†Ù…ÛŒâ€ŒØ´Ù‡.

Ø±Ø§Ù‡Ú©Ø§Ø±: ÛŒØ§ session support Ø±Ùˆ fully implement Ú©Ù† ÛŒØ§ endpoints Ø±Ùˆ conditional register Ú©Ù†:

python
if SESSION_SUPPORT:
    @app.post("/api/sessions")
    async def create_session(...):
        ...
9. HybridRetrievalAgent Crash Ø¨Ø§ Empty DB ğŸš¨
Ù…Ú©Ø§Ù†: agents/hybrid_retrieval.py Ø®Ø·ÙˆØ· 176-179

Ù…Ø´Ú©Ù„:

python
all_docs = getattr(self.vs, 'documents', [])
all_metas = getattr(self.vs, 'metadatas', [])
Ø§Ú¯Ù‡ vector store empty Ø¨Ø§Ø´Ù‡ Ùˆ documents attribute Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡ØŒ getattr empty list return Ù…ÛŒâ€ŒÚ©Ù†Ù‡. Ø§Ù…Ø§ Ø§Ú¯Ù‡ vector_store.search() call Ø¨Ø´Ù‡ Ù‚Ø¨Ù„Ø´ Ùˆ exception Ø¨Ø¯Ù‡ØŒ handle Ù†Ù…ÛŒâ€ŒØ´Ù‡.

Ø±Ø§Ù‡Ú©Ø§Ø±:

python
def _keyword_search(self, query: str, k: int) -> List[SearchResult]:
    results = []
    
    try:
        all_docs = getattr(self.vs, 'documents', [])
        all_metas = getattr(self.vs, 'metadatas', [])
        
        if not all_docs:
            logger.warning("Vector store is empty")
            return results
        
        # Rest of code...
    
    except Exception as e:
        logger.error(f"Keyword search failed: {e}")
        return results
ğŸ“‰ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ú©ÛŒÙÛŒØª (Quality Issues)
1. Chunk Size Ú©ÙˆÚ†Ú© âš ï¸
Ù…Ú©Ø§Ù†: ingestion.py Ø®Ø· 25ØŒ api_server.py Ø®Ø· 84

python
processor = DocumentProcessor(vector_store, chunk_size=500, chunk_overlap=50)
Ù…Ø´Ú©Ù„:

500 words Ø¨Ø±Ø§ÛŒ technical documents Ú©Ù…Ù‡

Context window 2048 tokens â†’ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… chunks Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…

Overlap 50 Ø®ÛŒÙ„ÛŒ Ú©Ù…Ù‡ (10%)

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯:

python
chunk_size=800  # 20% Ø§ÙØ²Ø§ÛŒØ´
chunk_overlap=160  # 20% overlap
2. Top-K Ú©Ù… Ø¯Ø± Retrieval âš ï¸
Ù…Ú©Ø§Ù†: main_engine.py Ø®Ø· 250ØŒ agents/specific_agents.py Ø®Ø· 117

python
k = context.get("top_k", 10)  # Default 10
Ù…Ø´Ú©Ù„: Ø¨Ø±Ø§ÛŒ queries Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ context Ø²ÛŒØ§Ø¯ Ø¯Ø§Ø±Ù†ØŒ 10 chunks Ú©Ù…Ù‡.

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯:

python
# Dynamic top-k based on query complexity
query_words = len(query.split())
k = 15 if query_words > 10 else 10
3. Verification Agent Ø¶Ø¹ÛŒÙ âš ï¸
Ù…Ú©Ø§Ù†: agents/specific_agents.py Ø®Ø·ÙˆØ· 641-707

Ù…Ø´Ú©Ù„: verification ÙÙ‚Ø· Ø§Ø² LLM Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ØŒ Ù‡ÛŒÚ† heuristic ÛŒØ§ fact-checking Ù†Ø¯Ø§Ø±Ø¯:

python
# Just asks LLM "is this correct?"
confidence = float(conf_str)  # Could be anything
Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Add heuristics:

python
def _calculate_confidence(self, answer, context, llm_confidence):
    # Heuristic 1: Answer length
    if len(answer) < 50:
        llm_confidence *= 0.8
    
    # Heuristic 2: Source overlap
    answer_words = set(answer.lower().split())
    context_words = set(' '.join(context).lower().split())
    overlap = len(answer_words & context_words) / len(answer_words)
    overlap_score = min(overlap, 1.0)
    
    # Heuristic 3: Citation presence
    has_citations = "source:" in answer.lower() or "page" in answer.lower()
    citation_bonus = 1.1 if has_citations else 1.0
    
    # Combine
    final_confidence = llm_confidence * overlap_score * citation_bonus
    return min(final_confidence, 1.0)
4. No Reranking âš ï¸
Ù…Ú©Ø§Ù†: agents/hybrid_retrieval.py Ø®Ø·ÙˆØ· 232-282

Ù…Ø´Ú©Ù„: Ø¨Ø¹Ø¯ Ø§Ø² mergeØŒ ÙÙ‚Ø· sort by weighted score Ù…ÛŒâ€ŒØ´Ù‡. Ù‡ÛŒÚ† cross-encoder reranking Ù†ÛŒØ³Øª.

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Add reranking stage:

python
from sentence_transformers import CrossEncoder

class HybridRetrievalAgent:
    def __init__(self, vector_store, config=None):
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def _rerank(self, query: str, results: List[SearchResult], top_k: int):
        # Score each result
        pairs = [(query, r.document) for r in results]
        scores = self.reranker.predict(pairs)
        
        # Combine with original scores
        for result, score in zip(results, scores):
            result.score = (result.score + score) / 2
        
        # Re-sort
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:top_k]
Ù†Ú©ØªÙ‡: Ø§ÛŒÙ† ÛŒÚ© dependency Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯:

bash
pip install sentence-transformers
5. Image Retrieval ÙÙ‚Ø· Caption-Based âš ï¸
Ù…Ú©Ø§Ù†: ingestion.py Ø®Ø·ÙˆØ· 265-297

Ù…Ø´Ú©Ù„: images ÙÙ‚Ø· based on text caption search Ù…ÛŒâ€ŒØ´Ù†. Ø§Ú¯Ù‡ caption Ø¶Ø¹ÛŒÙ Ø¨Ø§Ø´Ù‡ØŒ image Ù¾ÛŒØ¯Ø§ Ù†Ù…ÛŒâ€ŒØ´Ù‡.

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Add visual similarity search Ø¨Ø§ CLIP:

python
from sentence_transformers import util
import torch
from PIL import Image

class VisualSearchEngine:
    def __init__(self):
        from transformers import CLIPProcessor, CLIPModel
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.image_embeddings = {}
    
    def index_image(self, image_path: str):
        image = Image.open(image_path)
        inputs = self.processor(images=image, return_tensors="pt")
        with torch.no_grad():
            embedding = self.model.get_image_features(**inputs)
        self.image_embeddings[image_path] = embedding
    
    def search_by_text(self, query: str, top_k=5):
        inputs = self.processor(text=[query], return_tensors="pt")
        with torch.no_grad():
            text_embedding = self.model.get_text_features(**inputs)
        
        scores = {}
        for path, img_emb in self.image_embeddings.items():
            similarity = util.cos_sim(text_embedding, img_emb).item()
            scores[path] = similarity
        
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
6. No Query Expansion âš ï¸
Ù…Ú©Ø§Ù†: main_engine.py - orchestrator flow

Ù…Ø´Ú©Ù„: Ø§Ú¯Ù‡ query ambiguous Ø¨Ø§Ø´Ù‡ ÛŒØ§ synonyms Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡ØŒ retrieval weak Ù…ÛŒâ€ŒØ´Ù‡.

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Add query expansion:

python
def _expand_query(self, query: str) -> List[str]:
    """Generate alternative query formulations."""
    expansions = [query]
    
    # Add synonyms
    synonyms = {
        'worst case': ['pessimistic scenario', 'maximum deviation'],
        'sensitivity': ['parametric variation', 'robustness'],
        # Add more domain-specific synonyms
    }
    
    query_lower = query.lower()
    for term, alternatives in synonyms.items():
        if term in query_lower:
            for alt in alternatives:
                expansions.append(query_lower.replace(term, alt))
    
    # Add question reformulation
    if query.startswith('what is'):
        expansions.append(query.replace('what is', 'define'))
    
    return expansions[:3]  # Max 3 variants

# Use in retrieval:
expanded_queries = self._expand_query(user_query)
all_results = []
for q in expanded_queries:
    results = self.retrieval_agent.execute({"user_query": q, "top_k": 5})
    all_results.extend(results['documents'])

# Deduplicate and merge
...
7. Citation System Ø³Ø§Ø¯Ù‡ âš ï¸
Ù…Ú©Ø§Ù†: agents/specific_agents.py Ø®Ø·ÙˆØ· 554-569

Ù…Ø´Ú©Ù„: citations ÙÙ‚Ø· Ø¯Ø± footer append Ù…ÛŒâ€ŒØ´Ù†:

python
answer += "\n\n**Sources:**\n" + "\n".join(citation_list)
Ø¨Ù‡ØªØ± Ø¨ÙˆØ¯ inline citations Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… Ù…Ø«Ù„: "AOCS stands for Attitude and Orbit Control System."
â€‹

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯:

python
def _add_inline_citations(self, answer: str, metadatas: List[Dict]) -> str:
    """Add inline citations to answer."""
    # Build citation map
    citations = {}
    for i, meta in enumerate(metadatas[:5], 1):
        filename = meta.get('filename', 'unknown')
        page = meta.get('page', 0) + 1
        citations[i] = f"{filename} (Page {page})"
    
    # Find sentences that need citation
    sentences = answer.split('. ')
    cited_answer = []
    
    for sentence in sentences:
        # Heuristic: if sentence has technical info, add citation
        if any(keyword in sentence.lower() for keyword in ['is defined', 'consists of', 'includes', 'section']):
            # Find most relevant source
            best_cite = 1  # Simple: use first source
            cited_answer.append(f"{sentence} [{best_cite}]")
        else:
            cited_answer.append(sentence)
    
    result = '. '.join(cited_answer)
    
    # Add footer
    result += "\n\n**References:**\n"
    for i, cite in citations.items():
        result += f"[{i}] {cite}\n"
    
    return result
âš¡ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ù¾Ø±ÙÙˆØ±Ù…Ù†Ø³
1. Parallel Agent Execution ğŸš€
Ù…Ú©Ø§Ù†: main_engine.py - sequential execution

Ù…Ø´Ú©Ù„: Ù‡Ù…Ù‡ agents Ø¨Ù‡ ØµÙˆØ±Øª serial run Ù…ÛŒâ€ŒØ´Ù† â†’ response time 30-40s

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ (Ø·Ø¨Ù‚ FIX.md):

python
import asyncio

async def run_query_async(self, user_query: str):
    loop = asyncio.get_event_loop()
    
    # Stage 1: Query understanding (fast)
    query_result = await loop.run_in_executor(None, self.query_agent.execute, ...)
    
    # Stage 2: Retrieval (parallel with other prep)
    retrieval_task = loop.run_in_executor(None, self.retrieval_agent.execute, ...)
    # Do other work here...
    retrieval_result = await retrieval_task
    
    # Stage 3: Reasoning
    reasoning_task = loop.run_in_executor(None, self.reasoning_agent.execute, ...)
    answer_result = await reasoning_task
    
    # Stage 4: Verification + Artifact (parallel)
    verify_task = loop.run_in_executor(None, self.verification_agent.execute, ...)
    artifact_task = loop.run_in_executor(None, self._detect_artifact_need, ...)
    
    verify, artifact = await asyncio.gather(verify_task, artifact_task)
    
    return {...}
Expected speedup: 30-40% reduction (30s â†’ 20s)

2. Enable Caching ğŸš€
Ù…Ú©Ø§Ù†: main_engine.py uncomment Ø®Ø·ÙˆØ· 178-187ØŒ 376-379

Ù…Ø´Ú©Ù„: cache layer ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡ Ø§Ù…Ø§ disabled Ø§Ø³Øª.

Ø±Ø§Ù‡Ú©Ø§Ø±: Uncomment + fix bugs:

python
# In run_query():
cached_response = self.cache.get(user_query)
if cached_response:
    print(f"âœ… CACHE HIT: Returning cached response")
    cached_response['from_cache'] = True
    return cached_response

# ... normal processing ...

# Cache successful responses
if response['success'] and response['confidence'] >= 0.7:
    self.cache.set(user_query, response)
    print(f"ğŸ’¾ CACHE: Response cached")
Expected speedup: Cached queries < 1s (vs 30s)

3. Streaming Response ğŸš€
Ù…Ú©Ø§Ù†: api_server.py - add new endpoint

Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ (Ø·Ø¨Ù‚ FIX.md):

python
from fastapi.responses import StreamingResponse
import json

@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        # Send status updates as query processes
        yield json.dumps({"type": "status", "stage": "understanding"}) + "\n"
        
        # ... process query ...
        
        yield json.dumps({"type": "status", "stage": "retrieval", "chunks": 10}) + "\n"
        
        # ... reasoning ...
        
        yield json.dumps({"type": "status", "stage": "reasoning"}) + "\n"
        
        # Stream answer word by word
        words = answer.split()
        for i in range(0, len(words), 5):
            chunk = ' '.join(words[i:i+5])
            yield json.dumps({"type": "content", "data": chunk}) + "\n"
            await asyncio.sleep(0.05)
        
        yield json.dumps({"type": "complete", "metadata": {...}}) + "\n"
    
    return StreamingResponse(generate(), media_type="application/x-ndjson")
Frontend Ø¨Ø§ÛŒØ¯ EventSource ÛŒØ§ fetch with streaming handle Ú©Ù†Ù‡.

Expected improvement: Perceived latency Ú©Ø§Ù‡Ø´ (user content Ø±Ùˆ Ø²ÙˆØ¯ØªØ± Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù‡)

ğŸ¯ Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª Ù¾Ø§Ø³Ø®
1. Add Context Window Management ğŸ“Š
python
def _manage_context_window(self, chunks: List[str], max_tokens=1500):
    """Intelligently select chunks within token budget."""
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    
    selected = []
    total_tokens = 0
    
    for chunk in chunks:
        tokens = len(tokenizer.encode(chunk))
        if total_tokens + tokens > max_tokens:
            break
        selected.append(chunk)
        total_tokens += tokens
    
    return selected, total_tokens
2. Add Confidence Calibration ğŸ“Š
python
def _calibrate_confidence(self, raw_confidence: float, metadata: Dict) -> float:
    """Adjust confidence based on metadata."""
    calibrated = raw_confidence
    
    # Reduce confidence if:
    # - Low retrieval scores
    if metadata.get('best_distance', 0) > 0.8:
        calibrated *= 0.9
    
    # - Few sources
    if metadata.get('num_sources', 0) < 3:
        calibrated *= 0.95
    
    # - Short answer
    if len(metadata.get('answer', '')) < 100:
        calibrated *= 0.9
    
    # Increase confidence if:
    # - Multiple agents agree
    if metadata.get('verification_passed', False):
        calibrated *= 1.05
    
    return min(calibrated, 1.0)
3. Add Answer Post-Processing ğŸ“Š
python
def _postprocess_answer(self, answer: str) -> str:
    """Clean and enhance answer."""
    # Remove repetition
    sentences = answer.split('. ')
    seen = set()
    unique = []
    for s in sentences:
        if s.strip() and s.strip() not in seen:
            seen.add(s.strip())
            unique.append(s)
    answer = '. '.join(unique)
    
    # Fix formatting
    answer = re.sub(r'\n{3,}', '\n\n', answer)  # Max 2 newlines
    answer = re.sub(r' +', ' ', answer)  # Remove extra spaces
    
    # Add structure
    if len(answer) > 500 and '##' not in answer:
        # Add section headers if missing
        paragraphs = answer.split('\n\n')
        if len(paragraphs) > 2:
            structured = f"## Overview\n\n{paragraphs[0]}\n\n## Details\n\n"
            structured += '\n\n'.join(paragraphs[1:])
            answer = structured
    
    return answer
ğŸ—ï¸ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ù…Ø¹Ù…Ø§Ø±ÛŒ
1. Add Health Monitoring Dashboard
python
# utils/health_monitor.py
class HealthMonitor:
    def get_system_metrics(self):
        return {
            "llm_latency_ms": self.measure_llm_latency(),
            "vector_db_size": self.vs.get_collection_count(),
            "cache_hit_rate": self.cache.get_stats()['reuse_rate'],
            "avg_response_time": self.avg_response_time,
            "error_rate_24h": self.error_rate
        }
Ø¯Ø± frontend Ù†Ø´ÙˆÙ† Ø¨Ø¯Ù‡:

xml
<div class="health-dashboard">
  LLM: ğŸŸ¢ 125ms | DB: 1,234 docs | Cache: 45% hit rate
</div>
2. Add Evaluation Suite
python
# tests/eval_suite.py
test_queries = [
    {"query": "What is AOCS?", "expected_keywords": ["attitude", "orbit", "control"]},
    {"query": "ÙØ§Ø² Ø¢Ø±Ø§Ù…Ø´ Ú†ÛŒØ³ØªØŸ", "expected_keywords": ["tranquilization", "phase"]},
    {"query": "Show me figures about AOCS", "expected_type": "image_retrieval"},
    # Add 20-30 test queries
]

def run_evaluation():
    results = []
    for test in test_queries:
        response = orchestrator.run_query(test['query'])
        
        # Check if expected keywords present
        score = sum(1 for kw in test['expected_keywords'] 
                   if kw.lower() in response['answer'].lower())
        
        results.append({
            "query": test['query'],
            "score": score / len(test['expected_keywords']),
            "confidence": response['confidence'],
            "time_seconds": response['execution_time']
        })
    
    # Report
    avg_score = sum(r['score'] for r in results) / len(results)
    print(f"Evaluation Score: {avg_score:.2%}")
    return results
3. Add Logging & Observability
python
# utils/logger.py
import logging
from datetime import datetime

class StructuredLogger:
    def __init__(self):
        logging.basicConfig(
            filename=f'logs/app_{datetime.now().strftime("%Y%m%d")}.log',
            level=logging.INFO,
            format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
        )
        self.logger = logging.getLogger("RAG_System")
    
    def log_query(self, query, result):
        self.logger.info(json.dumps({
            "event": "query_processed",
            "query": query[:100],
            "success": result['success'],
            "confidence": result.get('confidence'),
            "latency_ms": result.get('latency_ms'),
            "sources_used": result.get('num_sources'),
            "from_cache": result.get('from_cache', False)
        }))
ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
ğŸ”´ Priority 1 (Critical - Ø§Ù…Ø±ÙˆØ²):
Fix requirements.txt â†’ Add missing deps

Uncomment caching â†’ Enable cache layer

Fix LLM timeout â†’ Add retry + validation

Fix translation fallback â†’ Add deep-translator

ğŸŸ  Priority 2 (High - Ø§ÛŒÙ† Ù‡ÙØªÙ‡):
Increase chunk size â†’ 500â†’800 + overlap 50â†’160

Add reranking â†’ CrossEncoder after retrieval

Improve verification â†’ Add heuristics

Add health monitoring â†’ Dashboard endpoint

ğŸŸ¡ Priority 3 (Medium - Ù‡ÙØªÙ‡ Ø¨Ø¹Ø¯):
Add query expansion â†’ Synonyms + reformulation

Implement streaming â†’ Better UX

Add inline citations â†’ Ø¯Ø± Ù…ØªÙ† Ù¾Ø§Ø³Ø®
â€‹

Parallel execution â†’ Async orchestrator

ğŸŸ¢ Priority 4 (Nice to have):
CLIP visual search â†’ Image similarity

Evaluation suite â†’ Automated testing

Structured logging â†’ Observability

Table extraction â†’ pdfplumber (Ø§Ú¯Ù‡ NFPA document Ø¯Ø§Ø±ÛŒ)

ğŸ¯ Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª Ø¨Ø¹Ø¯ Ø§Ø² Fix:
âœ… Response time: 30-40s â†’ 15-20s (Ø¨Ø§ caching â†’ <1s)

âœ… Success rate: 75% â†’ 85-90%

âœ… Confidence accuracy: Â±15% â†’ Â±10%

âœ… Zero crashes Ø¨Ø§ empty DB ÛŒØ§ translation failure